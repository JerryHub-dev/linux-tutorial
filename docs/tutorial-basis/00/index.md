# 00. 计算机概论

## cpu: 负责大量运算
- 微指令集会导致 CPU 的性能
- 频率：CPU 每秒可以进行的工作次数，如 3.0 GHz 的 CPU 每秒可以进行 3*10 的 9 次方次
- CPU 外频与倍频

  各个组件通过南桥与北桥连接至 CPU，

  - 外频：CPU 与外部组件进行数据传输/运算时的速度
  - 倍频：CPU 内部用来加速工作性能的一个倍数

  两者相乘才是 CPU 的频率，比如  3.0 GHz 的外频是 333 MHz，倍频就是 9 （3.0 G = 333M*9，其中 1 G = 1000M）
- 超频

  倍频一般出厂已锁定，所以只能调整外频的数值，从而达到频率提高，但是不稳定
- 32 位与 64 位

  CPU 运算的数据是由内存提供的，内存与 CPU 通信速度靠外部频率，那么每次可以传送数据量的大小就是总线的功能了，每次传送的位数称为「总线宽度」，也就是每秒钟可以传输的最大数据量。

  目前常见的总线宽度就是 32 位与 64 位（bit）

  CPU 每次能够处理的数据量称为 **字组大小（word size）**，字组大小依据 CPU 的设计有 32 位与 64 位。

  所以早期 32 位 CPU 中只能最大支持 4 GB 的内存，就是因为每次解析数据量太小的缘故
- CPU 等级

  CPU 的引脚位不统一导致出现了  i386、i586 等名词出现，64 位 CPU 统称为 x86_64 等级

## 内存
个人计算机的内存主要组件为动态随机访问内存（Dynamic Random Access Memory，DRAM），
内存也分为频率/频宽，也就是有数据传输宽度。

- 双通道设计：一条内存数据宽度为 64，那么双通道则变为 64*2
- CPU 频率与内存的关系

  理论上应该一致为好，比如 CPU 外频为 333HMz，则应该选择内存外频为 333MHz 的较好

-  DRAM 与 SRAM

  DRAM 就是内存，SRAM 就是把内做到到 CPU 中，也就是常听说的 L2 缓存，由于内置在 CPU 内部，传输速度更快
- 只读存储器 ROM

  BIOS 中的数据就是存储在 ROM 中的，但是需要通电才会有数据持久效果，也就是为什么在台式机主板上会有一颗纽扣电池的原因了

## 硬盘

- 硬盘物理组成

  硬盘由许多盘片、机械手臂、磁头与主轴马达所组成。

  数据是写在具有磁性物质的盘片上，而读写主要是通过在机械手臂上的读取头（head）来完成。
  实际运行时，主轴马达让盘片转动，然后机械手臂可以伸展让读取头在盘片上面进行读写操作

  ![](./assets/markdown-img-paste-20190731233327376.png)
- 盘片上的数据

  ![](./assets/markdown-img-paste-20190731233814157.png)

  - 扇区：每个扇区大小为 512 bytes，该值固定不变
  - 磁道：扇区组成的一个圆叫做磁道（Track）
  - 柱面：多硬盘上同一位置的磁道组成一个柱面，柱面也是我们分隔磁盘的最小单位了
- 传输接口：不同的传输接口传输速度不同

  - IDE : 有跳针，理论上传输速度为每秒 133 MB
  - SATA：1 代每秒 150 MB，2 代每秒 300M
- 容量：硬盘可以算一种消耗品，所以需要注意数据的重要性，该备份还得备份
- 缓冲存储器

  类似一块小内存，作用是将常见数据缓存起来，加快访问。目前主流的产品可达 16 MB
- 转速：因为是利用主轴马达转动盘片来访问，所以转速快慢会影响到性能
- 运转须知：

  由于硬盘的构造，在读取数据时，避免硬盘抖动，造成磁头读写错误数据，
  也应该避免非正常拔出插头，因为机械手臂必须要归回原位，所以正常的关机方式或则弹出，有利于硬盘的保养，因为会让硬盘的机械手臂归回原位

## 数据表示方式

计算机只认识 0 和 1，那么文字是怎么记录的呢？这个就是编码系统的工作了

### 数字系统

早期计算机适用的是利用通电与否的特性的真空管，通电就是 1，没通就是 0，这个也被沿用至今，称为二进制（Binary）。

十进制：逢十进一，比如 3456 的意义为：

```
3456 = 3*10^3 + 4*10^2 + 5*10^1 + 6*10^0

注意：10^0（10 的 0 次方）等于 1，10^1=10，10^2=100
```

二进制：逢二进一，比如 1101010 的计算方式为

```
一共 7 位
1101010 = 1*2^6 + 1*2^5 + 0*2^4 + 1*2^3 + 0*2^2 + 1*2^1 + 0*2^0
106     = 64    +   32  + 0*16  +   8   + 0*4   + 2     + 0*1

之前只知道二进制就是这样翻倍的规则，没有想到就是这样计算的。
```

二进制是计算机的基础，类似的 8 进制也是一样的。

那么十进制转二进制怎么做？刚刚是乘法，现在使用除法取余数就行了，比如

```   
       除法      余数
106 = 106/ 2     0
    = 53 / 2     1
    = 26 / 2     0
    = 13 / 2     1
    = 6  / 2     0
    = 3  / 2     1
    =            1      -> 1 或则 0 是二进制中最小的单位了

    所以余数倒过来：1101010

```

计算机的加减乘除都是通过这样的机制来计算的，其他的还有补码等运算方式

### 文字编码
既然计算机都只有记录 0 或则 1，甚至记录数据都是使用 byte/bit 等单位来记录的， 1 byte = 8 bit （bit 也就是二进制中的位数）

那么当文字记录被存储时，实际上也是存储的 0 或则 1，再读取出来的话，就需要经过一个编码系统的处理才行。所谓的「编码系统」可以想象成是一个「字码对照表」，如下所示

十进制 | 对应字母
-------|---------
65     | A
66     | B
67     | C

存储的二进制，经过编码系统之后还原为对应的文字。

常用的英文编码表为 ASCII 系统，这个编码系统中，每个符号（英文、数字或符号等）都会占用 1 B（Byte，也就是 8 b 位） 的记录，因此总共有 2^8 = 256 中变化。

中文编码系统，如 big5 (简体是 gb2312) ，每个中文会占用 2 B，理论上会有 2^16=65536，也就是最多可达 6 万多个中文字。但是并非所有的位数都是用来运用为对照表，所以并不会有 6 万多字。目前 big5 只定义了一万三千多个中文字，所以有好多繁体字无法显示。

big5 对于某些数据库系统来说是有问题的，比如 「许」、「盖」等字会被识别为单/双引号，可写入，但是读取出来就有问题了，乱码了。英文中也会出现这种的情况。

为了解决这个问题，国际组织 ISO/ICE 制定了 Unicode 编码系统，常常称呼为 UTF-8 编码。这个编码打破了所有国家的不同编码问题。

## 软件程序运行

### 机器程序与编译程序
计算机只认识 0 或 1，而且最重要的运算与逻辑判断是在 CPU 内部，而 CPU 是有微指令集的，因此需要 CPU 帮忙工作的时候，就需要参考微指令集内容，然后编写让 CPU 能识别的指令码，CPU 就能运行了。

这个过程有几个点很麻烦：

- 需要了解机器语言：只认 1 或 0，那么你需要学习直接写给机器看的内容
- 了解所有硬件相关功能函数：比如 DVD 播放影片，那么就需要参考光驱的硬件信息，写给光驱能看懂的指令码
- 程序不具有可移植性：每个 CPU 都有独特的微指令集，每个硬件也有自己的功能函数，所以换一台计算机可能该程序就不能使用了。
- 程序具有专一性：因为这样的程序需要针对硬件功能函数来编写，所以开发了浏览器程序，再开发文件管理程序的话，就得重新参考硬件的相关功能函数来编写，因此计算机科学家设计出了让人类看得懂的程序语言，然后创造一种「编译器」来将程序语言转译为机器能看得懂得机器码。比如 java 、 C 、C++ 等语言

编译器有了，但是在这样的环境下还需要考虑整体的硬件系统来设计程序，举例来说明：当你将运行的数据写入内存中，你需要自己分配一个内存块，让自己的数据能够填充上去，所以还需要了解内存的地址是如何定位的。

为了克服硬件方面老是需要重复编写句柄的问题，所以就有操作系统（Operating System OS）出现了

### 操作系统

在早期想要让计算机执行程序就要参考一堆硬件功能函数，并且需要学习机器语言才能实现。同时每次写程序时都必须要重新改写，因为硬件与软件功能不见得都一致。

所以如果能将所有硬件都驱动，并且提供一个开放软件的参考接口来给工程师开放软件的话，哪开放软件不就变得非常简单了？那么这个就是操作系统。
